{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16d74b02",
   "metadata": {},
   "source": [
    "# English detector \n",
    "## English or not english ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd78e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "import nltk.data\n",
    "import random\n",
    "from pprint import pprint\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a08f9cf",
   "metadata": {},
   "source": [
    "## Downloading the corpus\n",
    "\n",
    "> For this project I have decide to use the Universal Declaration of the Human Right because it's translated in many languages, for this assesment it's very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5791a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package udhr2 to /home/stromax/nltk_data...\n",
      "[nltk_data]   Package udhr2 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('udhr2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cba4e3",
   "metadata": {},
   "source": [
    "**To see all the texts of the corpus we can use this :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e5d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#udhr.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c75ce6",
   "metadata": {},
   "source": [
    "## Udhr Languages :\n",
    "> For the project I decide to use four languages:  \n",
    "\n",
    "- English  \n",
    "- French  \n",
    "- Italian  \n",
    "- Esperanto  \n",
    "\n",
    ">In this section I get the words and the sentences of thoses texts.  \n",
    "I will explain later why I need to get the words and the sentences of the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c911441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English words : 1918\n",
      "French words : 2316\n",
      "Italian words : 2173\n",
      "Esperanto words : 1833 \n",
      "\n",
      "English sentences : 74\n",
      "French sentences : 74\n",
      "Italian sentences : 77\n",
      "Esperanto sentences : 75\n"
     ]
    }
   ],
   "source": [
    "#Words\n",
    "english_udhr_words = nltk.corpus.udhr2.words('eng.txt')\n",
    "french_udhr_words = nltk.corpus.udhr2.words('fra.txt')\n",
    "italian_udhr_words = nltk.corpus.udhr2.words('ita.txt')\n",
    "esperanto_udhr_words = nltk.corpus.udhr2.words('epo.txt')\n",
    "\n",
    "\n",
    "\n",
    "#Sentences\n",
    "english_udhr_sents = nltk.corpus.udhr2.sents('eng.txt')\n",
    "french_udhr_sents = nltk.corpus.udhr2.sents('fra.txt')\n",
    "italian_udhr_sents = nltk.corpus.udhr2.sents('ita.txt')\n",
    "esperanto_udhr_sents = nltk.corpus.udhr2.sents('epo.txt')\n",
    "\n",
    "#Printing the numbers of words in the different languages\n",
    "print(\"English words :\", len(english_udhr_words))\n",
    "print(\"French words :\", len(french_udhr_words))\n",
    "print(\"Italian words :\", len(italian_udhr_words))\n",
    "print(\"Esperanto words :\", len(esperanto_udhr_words),\"\\n\")\n",
    "\n",
    "#Printing the numbers of sentences in the different languages\n",
    "print(\"English sentences :\", len(english_udhr_sents))\n",
    "print(\"French sentences :\", len(french_udhr_sents))\n",
    "print(\"Italian sentences :\", len(italian_udhr_sents))\n",
    "print(\"Esperanto sentences :\", len(esperanto_udhr_sents))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdc21e",
   "metadata": {},
   "source": [
    "## Words and sentences\n",
    "\n",
    "> In this section I do two things : \n",
    "First, I add all words of the different languages in a list called *languages_words*.    \n",
    "Second, I do the same things for the sentences but I need to add label to each sentences in function of their language here is the labels used : \n",
    "\n",
    "- english   \n",
    "- french   \n",
    "- italian   \n",
    "- esperanto     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7eadfc7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "languages_words = []\n",
    "\n",
    "#Adding english sentences\n",
    "for en in english_udhr_words:\n",
    "   languages_words.append(en)\n",
    "    \n",
    "# Adding french sentences\n",
    "for fr in french_udhr_words:\n",
    "   languages_words.append(fr)\n",
    "    \n",
    "# Adding italian sentences\n",
    "for it in italian_udhr_words:\n",
    "   languages_words.append(it)\n",
    "\n",
    "# Adding italian sentences\n",
    "for ep in esperanto_udhr_words:\n",
    "   languages_words.append(ep)\n",
    "\n",
    "languages_sents = []\n",
    "\n",
    "#Adding english sentences\n",
    "for en in english_udhr_sents:\n",
    "   languages_sents.append((en, 'english'))\n",
    "    \n",
    "# Adding french sentences\n",
    "for fr in french_udhr_sents:\n",
    "   languages_sents.append((fr, 'french'))\n",
    "    \n",
    "# Adding italian sentences\n",
    "for it in italian_udhr_sents:\n",
    "   languages_sents.append((it, 'italian'))\n",
    "\n",
    "# Adding italian sentences\n",
    "for ep in esperanto_udhr_sents:\n",
    "   languages_sents.append((ep, 'esperanto'))\n",
    "\n",
    "    \n",
    "#Shuffle the list of sentences\n",
    "random.shuffle(languages_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a27989",
   "metadata": {},
   "source": [
    "### Creating the features\n",
    "\n",
    "> Now I will use this function to format a sentences in a way that they can be used by the classifier.  \n",
    "The reason why I needed a list of words is because this list will be used as keys for this function.\n",
    "\n",
    ">The function will compare the list of words and the list of sentences to tell what words is contained in a sentence, if a word is contained it will render **True** otherwise **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa7d9271",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features = nltk.FreqDist(languages_words).keys()\n",
    "\n",
    "def extract_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features['contains(%s)' % word] = (word in document_words)\n",
    "    return features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ef878",
   "metadata": {},
   "source": [
    "## Splitting the list to obtain the train and the test set\n",
    "\n",
    "> Here I will split the features set previously obtained to get a train set and a test set the distribution will be as follows :  \n",
    ">- **80%** of the feature set for the train set  \n",
    ">- **20%** of the feature set for the test set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b6864b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in the training set : 240\n",
      "Number of sentences in the test set : 60\n"
     ]
    }
   ],
   "source": [
    "size_80 = int(len(languages_sents) * 0.8)\n",
    "\n",
    "train_set = languages_sents[:size_80]\n",
    "test_set = languages_sents[size_80:]\n",
    "\n",
    "print(\"Number of sentences in the training set :\",len(train_set))\n",
    "print(\"Number of sentences in the test set :\",len(test_set))\n",
    "\n",
    "train_featured_set = nltk.classify.apply_features(extract_features, train_set)\n",
    "test_featured_set = nltk.classify.apply_features(extract_features, test_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56de57",
   "metadata": {},
   "source": [
    "### Preparing the training of the classifier\n",
    "\n",
    "> And finally I will train the classifier !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669f696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_featured_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c43ff0",
   "metadata": {},
   "source": [
    "# Testing the classifier\n",
    "\n",
    "> Now that the classifier is trained I have done some examples to the reliability of it, I have also shown the accuracy calculated by nltk for the train set and the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a78d3e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train set : 0.9291666666666667\n",
      "Accuracy test set : 0.9\n",
      "english\n",
      "french\n",
      "italian\n",
      "esperanto\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Accuracy train set :\",nltk.classify.accuracy(classifier, train_featured_set))\n",
    "print(\"Accuracy test set :\",nltk.classify.accuracy(classifier, test_featured_set))\n",
    "\n",
    "t1 = \"The sudden departure of the head of trust and safety, the chief information security officer, and both the chief privacy and compliance officers is a dramatic development. It's not clear how soon they will be replaced, while the firm remains this unstable and sweeping job cuts have already been made.\"\n",
    "t2 = \"La presqu’île de Gâvres, près de Lorient (Morbihan), est l’un des sites de Bretagne les plus vulnérables face à la montée des eaux. Selon les scientifiques, la route qui la relie au continent sera sous l’eau en 2100 et une centaine de maisons disparaîtront. Mais la commune continue d’attirer et projette de construire 37 maisons.\"\n",
    "t3 = \"Italia, che spesso lo acquisiscono e lo usano insieme alle varianti regionali dell'italiano, alle lingue regionali e ai dialetti. In Italia viene ampiamente usato per tutti i tipi di comunicazione della vita quotidiana ed è largamente prevalente nei mezzi di comunicazione nazionali, nell'amministrazione pubblica dello Stato italiano e nell'editoria.\"\n",
    "t4 = \"Esperanto estas la plej sukcesa konstruita internacia helplingvo, kaj la sola tia lingvo kun ampleksa loĝantaro de denaskaj parolantoj, el kiuj estas eble kelkaj miloj. Uzadotaksoj estas malfacilaj, sed du taksoj metas la nombron de homoj kiuj scipovas paroli Esperanton je proksimume 100 000. Koncentriĝo de parolantoj estas plej alta en Eŭropo, Orienta Azio, kaj Sudameriko.\"\n",
    "\n",
    "\n",
    "print(classifier.classify(extract_features(t1.split())))\n",
    "print(classifier.classify(extract_features(t2.split())))\n",
    "print(classifier.classify(extract_features(t3.split())))\n",
    "print(classifier.classify(extract_features(t4.split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a49467",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "> For the end I will do a confusion matrix but for this I need two arrays : \n",
    "- gold :  which will be the true values of the confusion matrix\n",
    "- system : which will be the values of the system\n",
    "\n",
    ">I fill the gold array with the train set( named *train_featured_set*) because this is the values that have been trained with the classifier and I fill the system array with the test set(named *test_featured_set*).\n",
    "\n",
    ">After that I just need to use the confusion matrix method from nltk.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "567fc42f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "120\n",
      "          |     e       |\n",
      "          |     s       |\n",
      "          |  e  p     i |\n",
      "          |  n  e  f  t |\n",
      "          |  g  r  r  a |\n",
      "          |  l  a  e  l |\n",
      "          |  i  n  n  i |\n",
      "          |  s  t  c  a |\n",
      "          |  h  o  h  n |\n",
      "----------+-------------+\n",
      "  english |<14> 4  6 12 |\n",
      "esperanto |  6 <4> 6 10 |\n",
      "   french |  6 10 <8> 6 |\n",
      "  italian |  8  6  6 <8>|\n",
      "----------+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gold = []\n",
    "\n",
    "for i in range(len(train_featured_set)):\n",
    "    for j in range(len(train_featured_set[i])):\n",
    "        gold.append(train_featured_set[i][1])\n",
    "\n",
    "system = []\n",
    "\n",
    "for i in range(len(test_featured_set)):\n",
    "    for j in range(len(test_featured_set[i])):\n",
    "        system.append(test_featured_set[i][1])\n",
    "\n",
    "print(len(gold))\n",
    "print(len(system))\n",
    "\n",
    "cm = nltk.ConfusionMatrix(gold[:len(test_featured_set)*2], system)\n",
    "\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415144eb",
   "metadata": {},
   "source": [
    "## The precision, the accuracy and the recall\n",
    "\n",
    "> Finally we will measures the precision, the accuracy and the recall but for this we need to get the values of true positives, true negatives, false positives and false negatives.\n",
    "\n",
    ">For that I create a set which contain the labels and I go through it with 4 loops : \n",
    ">The first loop (i) get a row value ( for example **English**).\n",
    "\n",
    ">The second loop (j) will go through that row and when i and j is equal *( For example i is **English** and j is **English** )* I add the value of the confusion matrix to the true positives.   \n",
    "If i and j are not equal *( For example i is **English** and j is **French** )* then I add the value of the confusion matrix to the \"i\" values of the false negatives ( because false negatives are on rows) and I add the values of the confusion matrix to the \"j\" values of the false positives (because false positives are on columns).\n",
    "\n",
    "> The third loop and fourth loop is used when we have a true positive, at this moment we will go through the confusion matrix another time, k will works in the same way of i and l will works in the same way of j. To add  the true negative I proceed in this way, a true negative value is a value that is not a true positive, a false negative and a false postive, for the confusion matrix I have deduced that this a value which dont have **English** for i and j, for example. So a true negative will be add when the values of k and l will be different for i and j ( For example i and j is **English** so when k and l is different from **English** then we add a true negative values for **English**).\n",
    "\n",
    "\n",
    "> After that I have all of the values needed to calculated the precision, the accuracy and the recall with thoses formulas:  \n",
    "- Precision : **True positive / True positive + False positive**  \n",
    "- Recall : **True positive / True positive + False negative**  \n",
    "- Accuracy : **True positive + True negative/ True positive + True negative + False positive + False negative**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "299e5422",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.2833333333333333\n",
      "Recall : 0.2833333333333333\n",
      "Accuracy : 0.6416666666666667\n"
     ]
    }
   ],
   "source": [
    "labels = set('french english esperanto italian'.split())\n",
    "\n",
    "true_pos = Counter()\n",
    "true_neg = Counter()\n",
    "false_pos = Counter() \n",
    "false_neg = Counter() \n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_pos[i] += cm[i,j]\n",
    "            for k in labels:\n",
    "                if k != i:\n",
    "                    for l in labels:\n",
    "                        if k!=i and l!= j:\n",
    "                            true_neg[i]+= cm[k,l]\n",
    "        else:\n",
    "            false_neg[i] += cm[i,j]\n",
    "            false_pos[j] += cm[i,j]\n",
    "            \n",
    "\n",
    "true_pos_total = sum(true_pos.values())\n",
    "true_neg_total = sum(true_neg.values())\n",
    "false_pos_total = sum(false_pos.values())\n",
    "false_neg_total = sum(false_neg.values())\n",
    "\n",
    "\n",
    "print(\"Precision :\",(true_pos_total/(true_pos_total+false_pos_total)))\n",
    "print(\"Recall :\",(true_pos_total/(true_pos_total+false_neg_total)))\n",
    "print(\"Accuracy :\",((true_pos_total+ true_neg_total)/(true_pos_total+true_neg_total+false_neg_total+false_neg_total)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78336d96",
   "metadata": {},
   "source": [
    "***That's All Folks !***  \n",
    "Alan Vadeleau - French erasmus student\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
